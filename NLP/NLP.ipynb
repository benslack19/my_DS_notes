{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes to get keywords and use natural language processing**\n",
    "\n",
    "original application is for skills gap app\n",
    "\n",
    "**This is in the base python environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T03:06:39.236802Z",
     "start_time": "2020-09-12T03:06:28.177734Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T03:10:00.397240Z",
     "start_time": "2020-09-12T03:09:56.915692Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T03:10:05.054350Z",
     "start_time": "2020-09-12T03:10:01.179295Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of python tools\n",
    "\n",
    "My objective is an application for the skills gap app but as I'm learning more about the tools, I'm seeing that each serves different purposes. Here are notes for each.\n",
    "\n",
    "\n",
    "- Keywords: scikit-learn's CountVectorizer with TfidfTransformer or TfidfVectorizer\n",
    "- Natural language: NLTK and/or TextBlob\n",
    "- Word embedding: Word2vec (mapping of words into vectors of real numbers; apple, mango, banana should be placed close whereas books will be far away from these words).\n",
    "- Get better at regular expressions\n",
    "\n",
    "\n",
    "I think my closest application would be natural language but these notes will evolve as I learn more. Per [this post](https://www.quora.com/What-is-the-use-of-NLTK-and-TextBlob-What-is-the-difference-between-both-And-for-text-analysis-which-tool-is-better), there was this quote:\n",
    "\n",
    ">\"If you are new to this field I would advise you to start with NLTK and learn all its concepts which will build up all the basics required in this domain followed by moving towards Sentiment Analysis, Text Classification, Speech Recognition and Question Answering using TextBlob, Scikit-learn, Spacy and Stanford-OpenIE.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the DataSchool [tutorial](https://www.dataschool.io/learn/) does natural language with the scikit-learn packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T01:24:24.543737Z",
     "start_time": "2020-09-12T01:24:24.067905Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a052efba3d1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Word document of resume\n",
    "my_resume = '/Users/lacar/Documents/Goals_and_careers/Education Data Science/Udemy/BL_Resume_UDMY.docx'\n",
    "my_resume_docx_k = '/Users/lacar/Desktop/Kathleen/Lacar_Resume_Research_2018.docx'\n",
    "\n",
    "my_resume_pdf = '/Users/lacar/Documents/Goals_and_careers/Education Data Science/Udemy/BL_Resume_UDMY.pdf'\n",
    " \n",
    "# Link to target JD (or saved html)\n",
    "target_jd = 'https://jobs.lever.co/udemy/6b2e3401-bbd9-48f6-b6d8-e1fa19d801a7'\n",
    "# target_jd = '/Users/lacar/Documents/Goals_and_careers/Education Data Science/Udemy/Udemy\\ -\\ Data Scientist\\ -\\ Insights.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Opening resume with docx\n",
    "# https://stackoverflow.com/questions/25228106/how-to-extract-text-from-an-existing-docx-file-using-python-docx\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "resume_text_frdocx = getText(my_resume)\n",
    "# Cleanup new line, tab, return characters\n",
    "resume_text_frdocx = resume_text_frdocx.replace('\\n', ' ').replace('\\uf09f','').replace('\\uf020','').replace('\\t', '')\n",
    "\n",
    "resume_text_frdocx_k = getText(my_resume_docx_k)\n",
    "# Cleanup new line, tab, return characters\n",
    "resume_text_frdocx_k = resume_text_frdocx_k.replace('\\n', ' ').replace('\\uf09f','').replace('\\uf020','').replace('\\t', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Opening resume with pypdf2\n",
    "# https://medium.com/@rqaiserr/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f\n",
    "\n",
    "def return_text_fr_pdf(resume_as_pdf):\n",
    "    resume_pdf = open(resume_as_pdf, 'rb')\n",
    "    resume_pdf_reader = PyPDF2.PdfFileReader(resume_pdf)  # readable object\n",
    "    num_pages = resume_pdf_reader.numPages\n",
    "    count = 0\n",
    "    resume_text_frpdf = ''\n",
    "    # Use while loop to read each page\n",
    "    while count < num_pages:\n",
    "        page_obj = resume_pdf_reader.getPage(count)\n",
    "        count +=1\n",
    "        resume_text_frpdf += page_obj.extractText()\n",
    "    return resume_text_frpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text_frpdf = return_text_fr_pdf(my_resume_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4435"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resume_text_frdocx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_random_section_of_text(text, string_length):\n",
    "    start, end = 0, 0\n",
    "    while (end + string_length) < len(text):\n",
    "        start = int(round((np.random.random(1)*len(text))[0]))\n",
    "        end = start + string_length\n",
    "        substring_text = text[start:end]\n",
    "    return substring_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of Learning Center Trainee Co-Chair (2014-15) James S. McDonnell Foundation Postdoctoral Fellowship (2014-15) Neuroplasticity of Aging Postdoctoral Training Grant (2012-13)  NSF Graduate Research Fellowship (14% applicant success rate) (2006-09) UC Education Abroad Program Scholarship – Sweden (2001-02) UCLA Provost’s Honors List (1998-2003)   Volunteer Work and Activities                Provide guest lectures for UCSD Extension Stem Cell Biology Served as Neuroscience Consultant to Educators on UCSD Distinguished Educator Panel Toastmasters New Haven Hill Neighborhood Mentoring Program'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_random_section_of_text(resume_text_frdocx, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pa) Member of Psi Chi, National Honor Society in Psychology Howard D. Baker Undergraduate Research Award, third place Dean’s List, 2011-2012; 2002-2006 President’s List, 2011-2012; 2002-2006  CREDENTIALS/CERTIFICATIONS California State Registered Nurse  CCRN (Adult) certified through the American Association of Critical-Care Nurses ACLS and BLS certified through the American Heart Association  PALS certified through the American Heart Association NIH Stroke Scale Certified Data Science in Stratified Healthcare and Precision Medicine (The University of Edinburgh via Coursera)  '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_random_section_of_text(resume_text_frdocx_k, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening JD html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = target_jd\n",
    "document = urllib.request.urlopen(url).read().decode()\n",
    "\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "body = soup.body\n",
    "jd_text = body.get_text(separator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_text = jd_text.replace('\\n', ' ').replace('\\\\n', ' ').replace('\\xa0', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oration, and we share a serious belief in the power of learning and teaching to change lives. Udemy’s culture encourages innovation, creativity, passion, and teamwork. We also celebrate our milestones and support each other every day.  Founded in 2010, Udemy is privately owned and headquartered in San Francisco’s SOMA neighborhood with offices in Denver (Colorado), Dublin (Ireland), Ankara (Turkey), and São Paulo (Brazil).    Udemy in the News: The Key To Solving Future Skills Challenges Algorithms are coming for their jobs, so workers are teaching themselves algorithms Distractions Are Costing Companies Millions. Here\\'s Why 66 Percent of Workers Won\\'t Talk About It How Soft Skills Can Help You Get Ahead in a Tech World\"} var gaCode = \"UA-114911611-1\"; var gaAllowLinker = false; (function(i,s,o,g,r,a,m){i[\\'GoogleAnalyticsObject\\']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.paren'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_random_section_of_text(jd_text, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword text extraction\n",
    "\n",
    "- CountVectorizer followed by TF-IDF transformer [(tutorial)](https://www.freecodecamp.org/news/how-to-extract-keywords-from-text-with-tf-idf-and-pythons-scikit-learn-b2a0f3d7e667/)\n",
    "    - TF = term frequency, number of times a term appears in a document - *note that CountVectorizer only does this*\n",
    "    - IDF = log(total number of documents / number of documents that contain a term)\n",
    "    - The principle of TF-DF is that a document is compared against a lot of other documents to see what's special about it; **it can't get keywords in a single document alone**\n",
    "\n",
    "- TF-IDF *vectorizer* (equivalent to first option) [scikit-learn documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "- RAKE [(tutorial](https://www.airpair.com/nlp/keyword-extraction-tutorial), [documentation)](https://pypi.org/project/rake-nltk/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantify with CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text\n",
    "skills = ['storytelling', 'data visualization', 'applied statistics', 'SQL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['applied', 'data', 'sql', 'statistics', 'storytelling', 'visualization']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get fitted vocabulary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform training data into a 'document-term matrix'\n",
    "simple_train_dtm = vect.transform(skills)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This converts sparse matrix to dense matrix\n",
    "simple_train_dtm.toarray()\n",
    "\n",
    "# Note the size of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>applied</th>\n",
       "      <th>data</th>\n",
       "      <th>sql</th>\n",
       "      <th>statistics</th>\n",
       "      <th>storytelling</th>\n",
       "      <th>visualization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>storytelling</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data visualization</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>applied statistics</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SQL</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    applied  data  sql  statistics  storytelling  \\\n",
       "storytelling              0     0    0           0             1   \n",
       "data visualization        0     1    0           0             0   \n",
       "applied statistics        1     0    0           1             0   \n",
       "SQL                       0     0    1           0             0   \n",
       "\n",
       "                    visualization  \n",
       "storytelling                    0  \n",
       "data visualization              1  \n",
       "applied statistics              0  \n",
       "SQL                             0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put into a dataframe to examine vocab and matrix together\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names(), index=skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row is a different document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the parameters that are being used from the fit (does in place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_example1 = 'We’re Excited About You Because You Have • 3+ years of relevant experience in a combination of business/analytics roles (e.g. data science/analytics, management consulting, or business operations) • Ability to communicate effectively with non-technical stakeholders • Strong business intuition and judgement • Expert level data storytelling and data visualization ability • Broad knowledge of applied statistics, experimental design, and causal inference • Expert SQL ability and proficiency with 1+ programming languages (e.g., R or Python) • Experience with BI tools (e.g. Looker, Chartio, or Tableau)'\n",
    "doc_example2 = 'I like baseball'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We’re Excited About You Because You Have • 3+ years of relevant experience in a combination of business/analytics roles (e.g. data science/analytics, management consulting, or business operations) • Ability to communicate effectively with non-technical stakeholders • Strong business intuition and judgement • Expert level data storytelling and data visualization ability • Broad knowledge of applied statistics, experimental design, and causal inference • Expert SQL ability and proficiency with 1+ programming languages (e.g., R or Python) • Experience with BI tools (e.g. Looker, Chartio, or Tableau)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.decode(doc_example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like baseball'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.decode(doc_example2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF vectorizer with simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cell, following this nice tutorial https://www.youtube.com/watch?v=4vT4fzjkGCQ\n",
    "d1 = 'the sky is blue'\n",
    "d2 = 'the sky is not blue'\n",
    "df_test = pd.DataFrame()\n",
    "df_test['text'] = [d1, d2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the sky is blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the sky is not blue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text\n",
       "0      the sky is blue\n",
       "1  the sky is not blue"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(max_df=0.99, stop_words=['the'], use_idf=True)   # note that use_idf by default is False\n",
    "X = tv.fit_transform(df_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return values of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not']\n"
     ]
    }
   ],
   "source": [
    "print(tv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.40546511])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inverse document requency vector\n",
    "tv.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'not': 0}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blue', 'is', 'sky'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.stop_words_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume keywords\n",
    "df_res = pd.DataFrame()\n",
    "df_res['text'] = [resume_text_frdocx, resume_text_frdocx_k, jd_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BENJAMIN LACAR, Ph.D. 619.419.6227  ben.lacar@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KATHLEEN LACAR BSN, RN, CCRN kmuller@gmail.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - Insights San Francisco, Calif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  BENJAMIN LACAR, Ph.D. 619.419.6227  ben.lacar@...\n",
       "1  KATHLEEN LACAR BSN, RN, CCRN kmuller@gmail.com...\n",
       "2  Data Scientist - Insights San Francisco, Calif..."
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer and ignore words that appear in 85% of documents\n",
    "tv = TfidfVectorizer(max_df=0.85, stop_words='english', use_idf=True)\n",
    "X = tv.fit_transform(df_res['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x774 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 857 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02',\n",
       " '02893b1abca5',\n",
       " '05',\n",
       " '09',\n",
       " '114911611',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '1508857700820',\n",
       " '1663',\n",
       " '1998',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '2006',\n",
       " '2007',\n",
       " '2009',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '29',\n",
       " '40',\n",
       " '419',\n",
       " '42424051',\n",
       " '444c',\n",
       " '5350',\n",
       " '591',\n",
       " '619',\n",
       " '6227',\n",
       " '66',\n",
       " '78ab6997',\n",
       " '850',\n",
       " '94080',\n",
       " '96',\n",
       " '98',\n",
       " 'aa',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'abroad',\n",
       " 'academic',\n",
       " 'accessibility',\n",
       " 'achievements',\n",
       " 'acls',\n",
       " 'act',\n",
       " 'actionable',\n",
       " 'activated',\n",
       " 'activities',\n",
       " 'acuity',\n",
       " 'ad',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'addresscountry',\n",
       " 'addresslocality',\n",
       " 'addressregion',\n",
       " 'adept',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advancing',\n",
       " 'adverse',\n",
       " 'affective',\n",
       " 'aging',\n",
       " 'ahead',\n",
       " 'algorithm',\n",
       " 'algorithms',\n",
       " 'allocation',\n",
       " 'allowlinker',\n",
       " 'amazonaws',\n",
       " 'american',\n",
       " 'analyses',\n",
       " 'analysis',\n",
       " 'analytics',\n",
       " 'analyze',\n",
       " 'analyzed',\n",
       " 'angeles',\n",
       " 'ankara',\n",
       " 'anonymizeip',\n",
       " 'answering',\n",
       " 'applicant',\n",
       " 'applications',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'apr',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'arguments',\n",
       " 'arts',\n",
       " 'assist',\n",
       " 'assistant',\n",
       " 'assisted',\n",
       " 'associate',\n",
       " 'association',\n",
       " 'assurance',\n",
       " 'async',\n",
       " 'audits',\n",
       " 'aug',\n",
       " 'authored',\n",
       " 'award',\n",
       " 'awardee',\n",
       " 'b150',\n",
       " 'bachelor',\n",
       " 'background',\n",
       " 'baker',\n",
       " 'base',\n",
       " 'bayesian',\n",
       " 'behavior',\n",
       " 'behaviorally',\n",
       " 'belief',\n",
       " 'believe',\n",
       " 'ben',\n",
       " 'benjamin',\n",
       " 'benslack19',\n",
       " 'beta',\n",
       " 'bi',\n",
       " 'biochemistry',\n",
       " 'bioinformaticians',\n",
       " 'bioinformatics',\n",
       " 'biological',\n",
       " 'biology',\n",
       " 'blood',\n",
       " 'bls',\n",
       " 'bolster',\n",
       " 'bootcamp',\n",
       " 'bottlenecks',\n",
       " 'brain',\n",
       " 'brazil',\n",
       " 'broad',\n",
       " 'bsn',\n",
       " 'build',\n",
       " 'built',\n",
       " 'bunch',\n",
       " 'business',\n",
       " 'businesses',\n",
       " 'ca',\n",
       " 'cabg',\n",
       " 'cancer',\n",
       " 'capay',\n",
       " 'capillary',\n",
       " 'cardiac',\n",
       " 'cardiothoracic',\n",
       " 'cardiovascular',\n",
       " 'care',\n",
       " 'careers',\n",
       " 'causal',\n",
       " 'causes',\n",
       " 'ccrn',\n",
       " 'celebrate',\n",
       " 'cell',\n",
       " 'cells',\n",
       " 'cellular',\n",
       " 'center',\n",
       " 'centered',\n",
       " 'certifications',\n",
       " 'certified',\n",
       " 'chair',\n",
       " 'challenges',\n",
       " 'change',\n",
       " 'changes',\n",
       " 'charting',\n",
       " 'chartio',\n",
       " 'chi',\n",
       " 'circle',\n",
       " 'classroom',\n",
       " 'client',\n",
       " 'clinical',\n",
       " 'close',\n",
       " 'code',\n",
       " 'cognition',\n",
       " 'cognitive',\n",
       " 'collaborate',\n",
       " 'collaborated',\n",
       " 'collaboration',\n",
       " 'collaborations',\n",
       " 'collected',\n",
       " 'colorado',\n",
       " 'combination',\n",
       " 'coming',\n",
       " 'commercial',\n",
       " 'communicate',\n",
       " 'communicating',\n",
       " 'communication',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'competitiveness',\n",
       " 'compiled',\n",
       " 'complete',\n",
       " 'completed',\n",
       " 'complex',\n",
       " 'compliance',\n",
       " 'concept',\n",
       " 'concepts',\n",
       " 'concerning',\n",
       " 'concurrent',\n",
       " 'conduct',\n",
       " 'conducted',\n",
       " 'confidential',\n",
       " 'connect',\n",
       " 'consented',\n",
       " 'consultant',\n",
       " 'consulting',\n",
       " 'context',\n",
       " 'continuously',\n",
       " 'contributing',\n",
       " 'conversion',\n",
       " 'conveys',\n",
       " 'coordinated',\n",
       " 'corporation',\n",
       " 'costing',\n",
       " 'council',\n",
       " 'course',\n",
       " 'coursera',\n",
       " 'courses',\n",
       " 'create',\n",
       " 'created',\n",
       " 'createelement',\n",
       " 'creation',\n",
       " 'creativity',\n",
       " 'creator',\n",
       " 'credentials',\n",
       " 'critical',\n",
       " 'cross',\n",
       " 'crrt',\n",
       " 'cruz',\n",
       " 'ct',\n",
       " 'culture',\n",
       " 'cum',\n",
       " 'customer',\n",
       " 'dashboard',\n",
       " 'dashboards',\n",
       " 'datasets',\n",
       " 'date',\n",
       " 'dateposted',\n",
       " 'day',\n",
       " 'dean',\n",
       " 'dec',\n",
       " 'decisions',\n",
       " 'define',\n",
       " 'defined',\n",
       " 'denver',\n",
       " 'depth',\n",
       " 'description',\n",
       " 'descriptions',\n",
       " 'design',\n",
       " 'designated',\n",
       " 'development',\n",
       " 'devices',\n",
       " 'diego',\n",
       " 'direct',\n",
       " 'direction',\n",
       " 'discovered',\n",
       " 'disease',\n",
       " 'dissertation',\n",
       " 'distinguished',\n",
       " 'distractions',\n",
       " 'diverse',\n",
       " 'dna',\n",
       " 'doctoral',\n",
       " 'document',\n",
       " 'documents',\n",
       " 'doing',\n",
       " 'domain',\n",
       " 'draw',\n",
       " 'drive',\n",
       " 'driven',\n",
       " 'dublin',\n",
       " 'dynamics',\n",
       " 'ecls',\n",
       " 'ecmo',\n",
       " 'edinburgh',\n",
       " 'education',\n",
       " 'educational',\n",
       " 'educator',\n",
       " 'educators',\n",
       " 'effectively',\n",
       " 'efforts',\n",
       " 'embedded',\n",
       " 'employmenttype',\n",
       " 'encourages',\n",
       " 'engineering',\n",
       " 'enjoys',\n",
       " 'ensure',\n",
       " 'entered',\n",
       " 'equipment',\n",
       " 'evangelist',\n",
       " 'excited',\n",
       " 'execute',\n",
       " 'executes',\n",
       " 'executive',\n",
       " 'experimental',\n",
       " 'expert',\n",
       " 'experts',\n",
       " 'exploring',\n",
       " 'exposure',\n",
       " 'expression',\n",
       " 'extension',\n",
       " 'extensive',\n",
       " 'external',\n",
       " 'face',\n",
       " 'facilitate',\n",
       " 'facing',\n",
       " 'false',\n",
       " 'family',\n",
       " 'fellow',\n",
       " 'fellowship',\n",
       " 'fellowships',\n",
       " 'field',\n",
       " 'fl',\n",
       " 'fl2002',\n",
       " 'fl2011',\n",
       " 'floating',\n",
       " 'florida',\n",
       " 'flow',\n",
       " 'fluidigm',\n",
       " 'focused',\n",
       " 'follow',\n",
       " 'foundation',\n",
       " 'founded',\n",
       " 'function',\n",
       " 'functional',\n",
       " 'functions',\n",
       " 'funds',\n",
       " 'funnels',\n",
       " 'future',\n",
       " 'ga',\n",
       " 'gaallowlinker',\n",
       " 'gacode',\n",
       " 'gene',\n",
       " 'genetics',\n",
       " 'genomics',\n",
       " 'getelementsbytagname',\n",
       " 'github',\n",
       " 'given',\n",
       " 'global',\n",
       " 'gmail',\n",
       " 'goaling',\n",
       " 'good',\n",
       " 'google',\n",
       " 'googleanalyticsobject',\n",
       " 'governments',\n",
       " 'gpa',\n",
       " 'graduate',\n",
       " 'grant',\n",
       " 'guest',\n",
       " 'guide',\n",
       " 'guidebook',\n",
       " 'haven',\n",
       " 'hay',\n",
       " 'headquartered',\n",
       " 'health',\n",
       " 'healthcare',\n",
       " 'heart',\n",
       " 'help',\n",
       " 'high',\n",
       " 'highlights',\n",
       " 'highly',\n",
       " 'hill',\n",
       " 'hippocampus',\n",
       " 'hiringorganization',\n",
       " 'history',\n",
       " 'hoc',\n",
       " 'home',\n",
       " 'honor',\n",
       " 'honors',\n",
       " 'hospital',\n",
       " 'howard',\n",
       " 'http',\n",
       " 'https',\n",
       " 'human',\n",
       " 'iabp',\n",
       " 'icu',\n",
       " 'identified',\n",
       " 'identifies',\n",
       " 'ii',\n",
       " 'image',\n",
       " 'imagine',\n",
       " 'implements',\n",
       " 'important',\n",
       " 'improve',\n",
       " 'include',\n",
       " 'including',\n",
       " 'individual',\n",
       " 'inference',\n",
       " 'influence',\n",
       " 'influenced',\n",
       " 'influences',\n",
       " 'inform',\n",
       " 'information',\n",
       " 'inherited',\n",
       " 'innovation',\n",
       " 'input',\n",
       " 'insertbefore',\n",
       " 'insight',\n",
       " 'insights',\n",
       " 'institute',\n",
       " 'instructors',\n",
       " 'intensive',\n",
       " 'interacting',\n",
       " 'interpersonal',\n",
       " 'interpretation',\n",
       " 'introduction',\n",
       " 'intuition',\n",
       " 'investigator',\n",
       " 'ireland',\n",
       " 'issues',\n",
       " 'james',\n",
       " 'jan',\n",
       " 'jewell',\n",
       " 'job',\n",
       " 'joblocation',\n",
       " 'jobposting',\n",
       " 'jobs',\n",
       " 'jolla',\n",
       " 'js',\n",
       " 'judgement',\n",
       " 'kappa',\n",
       " 'kathleen',\n",
       " 'key',\n",
       " 'kmuller',\n",
       " 'knit',\n",
       " 'knowledge',\n",
       " 'la',\n",
       " 'lab',\n",
       " 'laboratory',\n",
       " 'lacar',\n",
       " 'lane',\n",
       " 'languages',\n",
       " 'large',\n",
       " 'laude',\n",
       " 'launches',\n",
       " 'leadership',\n",
       " 'learning',\n",
       " 'lectures',\n",
       " 'legacy',\n",
       " 'lever',\n",
       " 'leveraged',\n",
       " 'liaison',\n",
       " 'library',\n",
       " 'life',\n",
       " 'linkedin',\n",
       " 'linkid',\n",
       " 'list',\n",
       " 'lives',\n",
       " 'll',\n",
       " 'local',\n",
       " 'logo',\n",
       " 'logos',\n",
       " 'looker',\n",
       " 'los',\n",
       " 'lung',\n",
       " 'machine',\n",
       " 'maglab',\n",
       " 'magnet',\n",
       " 'magnetic',\n",
       " 'managed',\n",
       " 'management',\n",
       " 'manager',\n",
       " 'managers',\n",
       " 'manuscripts',\n",
       " 'marion',\n",
       " 'market',\n",
       " 'marketing',\n",
       " 'marketplace',\n",
       " 'master',\n",
       " 'mastering',\n",
       " 'matlab',\n",
       " 'matter',\n",
       " 'mcdonnell',\n",
       " 'mechanical',\n",
       " 'medical',\n",
       " 'medicine',\n",
       " 'meetings',\n",
       " 'member',\n",
       " 'memorial',\n",
       " 'memory',\n",
       " 'mentoring',\n",
       " 'mentorship',\n",
       " 'messaging',\n",
       " 'method',\n",
       " 'methods',\n",
       " 'metrics',\n",
       " 'michigan',\n",
       " 'milestones',\n",
       " 'million',\n",
       " 'millions',\n",
       " 'model',\n",
       " 'monitor',\n",
       " 'monitoring',\n",
       " 'multiple',\n",
       " 'national',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neighborhood',\n",
       " 'network',\n",
       " 'neural',\n",
       " 'neurological',\n",
       " 'neurons',\n",
       " 'neuroplasticity',\n",
       " 'neuroscience',\n",
       " 'news',\n",
       " 'nih',\n",
       " 'non',\n",
       " 'note',\n",
       " 'nsf',\n",
       " 'null',\n",
       " 'nurse',\n",
       " 'nurses',\n",
       " 'nursing',\n",
       " 'offices',\n",
       " 'ongoing',\n",
       " 'online',\n",
       " 'operations',\n",
       " 'org',\n",
       " 'organ',\n",
       " 'organization',\n",
       " 'organizations',\n",
       " 'organize',\n",
       " 'organized',\n",
       " 'orthopedic',\n",
       " 'oversaw',\n",
       " 'owned',\n",
       " 'page',\n",
       " 'pageview',\n",
       " 'pals',\n",
       " 'panel',\n",
       " 'papers',\n",
       " 'parentnode',\n",
       " 'participants',\n",
       " 'passion',\n",
       " 'passions',\n",
       " 'patient',\n",
       " 'patients',\n",
       " 'paulo',\n",
       " 'people',\n",
       " 'percent',\n",
       " 'perform',\n",
       " 'ph',\n",
       " 'phi',\n",
       " 'phil',\n",
       " 'pipeline',\n",
       " 'pis',\n",
       " 'place',\n",
       " 'planetree',\n",
       " 'plotting',\n",
       " 'png',\n",
       " 'populations',\n",
       " 'postaladdress',\n",
       " 'postalcode',\n",
       " 'postdoctoral',\n",
       " 'potential',\n",
       " 'power',\n",
       " 'powered',\n",
       " 'practical',\n",
       " 'practices',\n",
       " 'preceptor',\n",
       " 'precisely',\n",
       " 'precision',\n",
       " 'preparation',\n",
       " 'prepared',\n",
       " 'present',\n",
       " 'presentation',\n",
       " 'presentations',\n",
       " 'presented',\n",
       " 'president',\n",
       " 'primary',\n",
       " 'principle',\n",
       " 'prioritization',\n",
       " 'privately',\n",
       " 'proactively',\n",
       " 'problem',\n",
       " 'procedures',\n",
       " 'process',\n",
       " 'produce',\n",
       " 'product',\n",
       " 'professional',\n",
       " 'proficiency',\n",
       " 'progenitor',\n",
       " 'program',\n",
       " 'programming',\n",
       " 'project',\n",
       " 'projects',\n",
       " 'proofed',\n",
       " 'protected',\n",
       " 'protocols',\n",
       " 'provide',\n",
       " 'provided',\n",
       " 'provides',\n",
       " 'provost',\n",
       " 'psi',\n",
       " 'psychology',\n",
       " 'publications',\n",
       " 'push',\n",
       " 'python',\n",
       " 'quality',\n",
       " 'quantitative',\n",
       " 'questions',\n",
       " 'range',\n",
       " 'rate',\n",
       " 'reactions',\n",
       " 'refer',\n",
       " 'refines',\n",
       " 'regeneration',\n",
       " 'region',\n",
       " 'registered',\n",
       " 'regulating',\n",
       " 'regulatory',\n",
       " 'relevant',\n",
       " 'repair',\n",
       " 'repairs',\n",
       " 'replacement',\n",
       " 'reported',\n",
       " 'representation',\n",
       " 'require',\n",
       " 'requirements',\n",
       " 'requires',\n",
       " 'research',\n",
       " 'researcher',\n",
       " 'resource',\n",
       " 'right',\n",
       " 'rn',\n",
       " 'rna',\n",
       " 'role',\n",
       " 'roles',\n",
       " 'runner',\n",
       " 's3',\n",
       " 'safety',\n",
       " 'salk',\n",
       " 'santa',\n",
       " 'scale',\n",
       " 'scan',\n",
       " 'schedule',\n",
       " 'schema',\n",
       " 'scholarship',\n",
       " 'scicd',\n",
       " 'scientific',\n",
       " 'scientist',\n",
       " 'scientists',\n",
       " 'scope',\n",
       " 'screened',\n",
       " 'script',\n",
       " 'scripted',\n",
       " 'segmenting',\n",
       " 'selected',\n",
       " 'send',\n",
       " 'sept',\n",
       " 'seq',\n",
       " 'sequencing',\n",
       " 'series',\n",
       " 'serve',\n",
       " 'served',\n",
       " 'services',\n",
       " 'set',\n",
       " 'sets',\n",
       " 'setting',\n",
       " 'settings',\n",
       " 'share',\n",
       " 'shared',\n",
       " 'sharp',\n",
       " 'sigma',\n",
       " 'signatures',\n",
       " 'single',\n",
       " 'skilled',\n",
       " 'smith',\n",
       " 'social',\n",
       " 'society',\n",
       " 'soft',\n",
       " 'solutions',\n",
       " 'solving',\n",
       " 'soma',\n",
       " 'south',\n",
       " 'specializing',\n",
       " 'sql',\n",
       " 'src',\n",
       " 'stacks',\n",
       " 'staff',\n",
       " 'staffing',\n",
       " 'stakeholders',\n",
       " 'standard',\n",
       " 'stanford',\n",
       " 'state',\n",
       " 'statistical',\n",
       " 'statistics',\n",
       " 'stem',\n",
       " 'storytelling',\n",
       " 'stratified',\n",
       " 'stroke',\n",
       " 'strong',\n",
       " 'strongly',\n",
       " 'student',\n",
       " 'students',\n",
       " 'studies',\n",
       " 'study',\n",
       " 'subject',\n",
       " 'subjects',\n",
       " 'success',\n",
       " 'summa',\n",
       " 'supervised',\n",
       " 'support',\n",
       " 'supports',\n",
       " 'surgery',\n",
       " 'sweden',\n",
       " 'são',\n",
       " 'tableau',\n",
       " 'talk',\n",
       " 'tallahassee',\n",
       " 'tau',\n",
       " 'teaching',\n",
       " 'team',\n",
       " 'teams',\n",
       " 'teamwork',\n",
       " 'tech',\n",
       " 'technical',\n",
       " 'technicians',\n",
       " 'temporal',\n",
       " 'theta',\n",
       " 'think',\n",
       " 'throughput',\n",
       " 'time',\n",
       " 'title',\n",
       " 'toastmasters',\n",
       " 'today',\n",
       " 'tool',\n",
       " 'tools',\n",
       " 'track',\n",
       " 'tracks',\n",
       " 'tracksidentify',\n",
       " 'trained',\n",
       " 'trainee',\n",
       " 'training',\n",
       " 'transplantation',\n",
       " 'transplants',\n",
       " 'trauma',\n",
       " 'treatment',\n",
       " 'treatments',\n",
       " 'true',\n",
       " 'turkey',\n",
       " 'type',\n",
       " 'ua',\n",
       " 'uc',\n",
       " 'ucla',\n",
       " 'ucsd',\n",
       " 'udemy',\n",
       " 'undergoing',\n",
       " 'undergraduate',\n",
       " 'undergraduates',\n",
       " 'unit',\n",
       " 'units',\n",
       " 'university',\n",
       " 'ups',\n",
       " 'usa',\n",
       " 'use',\n",
       " 'user',\n",
       " 'valve',\n",
       " 'var',\n",
       " 'variety',\n",
       " 'vascular',\n",
       " 'visualization',\n",
       " 'visualizations',\n",
       " 'volunteer',\n",
       " 'white',\n",
       " 'willingness',\n",
       " 'window',\n",
       " 'won',\n",
       " 'workers',\n",
       " 'working',\n",
       " 'world',\n",
       " 'www',\n",
       " 'yale',\n",
       " 'years']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'02': 0,\n",
       " '02893b1abca5': 1,\n",
       " '05': 2,\n",
       " '09': 3,\n",
       " '114911611': 4,\n",
       " '13': 5,\n",
       " '14': 6,\n",
       " '15': 7,\n",
       " '1508857700820': 8,\n",
       " '1663': 9,\n",
       " '1998': 10,\n",
       " '2001': 11,\n",
       " '2002': 12,\n",
       " '2003': 13,\n",
       " '2004': 14,\n",
       " '2006': 15,\n",
       " '2007': 16,\n",
       " '2009': 17,\n",
       " '2011': 18,\n",
       " '2012': 19,\n",
       " '2013': 20,\n",
       " '2014': 21,\n",
       " '2015': 22,\n",
       " '2017': 23,\n",
       " '2018': 24,\n",
       " '2019': 25,\n",
       " '29': 26,\n",
       " '40': 27,\n",
       " '419': 28,\n",
       " '42424051': 29,\n",
       " '444c': 30,\n",
       " '5350': 31,\n",
       " '591': 32,\n",
       " '619': 33,\n",
       " '6227': 34,\n",
       " '66': 35,\n",
       " '78ab6997': 36,\n",
       " '850': 37,\n",
       " '94080': 38,\n",
       " '96': 39,\n",
       " '98': 40,\n",
       " 'aa': 41,\n",
       " 'ability': 42,\n",
       " 'able': 43,\n",
       " 'abroad': 44,\n",
       " 'academic': 45,\n",
       " 'accessibility': 46,\n",
       " 'achievements': 47,\n",
       " 'acls': 48,\n",
       " 'act': 49,\n",
       " 'actionable': 50,\n",
       " 'activated': 51,\n",
       " 'activities': 52,\n",
       " 'acuity': 53,\n",
       " 'ad': 54,\n",
       " 'additional': 55,\n",
       " 'address': 56,\n",
       " 'addresscountry': 57,\n",
       " 'addresslocality': 58,\n",
       " 'addressregion': 59,\n",
       " 'adept': 60,\n",
       " 'adult': 61,\n",
       " 'advance': 62,\n",
       " 'advancing': 63,\n",
       " 'adverse': 64,\n",
       " 'affective': 65,\n",
       " 'aging': 66,\n",
       " 'ahead': 67,\n",
       " 'algorithm': 68,\n",
       " 'algorithms': 69,\n",
       " 'allocation': 70,\n",
       " 'allowlinker': 71,\n",
       " 'amazonaws': 72,\n",
       " 'american': 73,\n",
       " 'analyses': 74,\n",
       " 'analysis': 75,\n",
       " 'analytics': 76,\n",
       " 'analyze': 77,\n",
       " 'analyzed': 78,\n",
       " 'angeles': 79,\n",
       " 'ankara': 80,\n",
       " 'anonymizeip': 81,\n",
       " 'answering': 82,\n",
       " 'applicant': 83,\n",
       " 'applications': 84,\n",
       " 'applied': 85,\n",
       " 'apply': 86,\n",
       " 'applying': 87,\n",
       " 'apr': 88,\n",
       " 'area': 89,\n",
       " 'areas': 90,\n",
       " 'arguments': 91,\n",
       " 'arts': 92,\n",
       " 'assist': 93,\n",
       " 'assistant': 94,\n",
       " 'assisted': 95,\n",
       " 'associate': 96,\n",
       " 'association': 97,\n",
       " 'assurance': 98,\n",
       " 'async': 99,\n",
       " 'audits': 100,\n",
       " 'aug': 101,\n",
       " 'authored': 102,\n",
       " 'award': 103,\n",
       " 'awardee': 104,\n",
       " 'b150': 105,\n",
       " 'bachelor': 106,\n",
       " 'background': 107,\n",
       " 'baker': 108,\n",
       " 'base': 109,\n",
       " 'bayesian': 110,\n",
       " 'behavior': 111,\n",
       " 'behaviorally': 112,\n",
       " 'belief': 113,\n",
       " 'believe': 114,\n",
       " 'ben': 115,\n",
       " 'benjamin': 116,\n",
       " 'benslack19': 117,\n",
       " 'beta': 118,\n",
       " 'bi': 119,\n",
       " 'biochemistry': 120,\n",
       " 'bioinformaticians': 121,\n",
       " 'bioinformatics': 122,\n",
       " 'biological': 123,\n",
       " 'biology': 124,\n",
       " 'blood': 125,\n",
       " 'bls': 126,\n",
       " 'bolster': 127,\n",
       " 'bootcamp': 128,\n",
       " 'bottlenecks': 129,\n",
       " 'brain': 130,\n",
       " 'brazil': 131,\n",
       " 'broad': 132,\n",
       " 'bsn': 133,\n",
       " 'build': 134,\n",
       " 'built': 135,\n",
       " 'bunch': 136,\n",
       " 'business': 137,\n",
       " 'businesses': 138,\n",
       " 'ca': 139,\n",
       " 'cabg': 140,\n",
       " 'cancer': 141,\n",
       " 'capay': 142,\n",
       " 'capillary': 143,\n",
       " 'cardiac': 144,\n",
       " 'cardiothoracic': 145,\n",
       " 'cardiovascular': 146,\n",
       " 'care': 147,\n",
       " 'careers': 148,\n",
       " 'causal': 149,\n",
       " 'causes': 150,\n",
       " 'ccrn': 151,\n",
       " 'celebrate': 152,\n",
       " 'cell': 153,\n",
       " 'cells': 154,\n",
       " 'cellular': 155,\n",
       " 'center': 156,\n",
       " 'centered': 157,\n",
       " 'certifications': 158,\n",
       " 'certified': 159,\n",
       " 'chair': 160,\n",
       " 'challenges': 161,\n",
       " 'change': 162,\n",
       " 'changes': 163,\n",
       " 'charting': 164,\n",
       " 'chartio': 165,\n",
       " 'chi': 166,\n",
       " 'circle': 167,\n",
       " 'classroom': 168,\n",
       " 'client': 169,\n",
       " 'clinical': 170,\n",
       " 'close': 171,\n",
       " 'code': 172,\n",
       " 'cognition': 173,\n",
       " 'cognitive': 174,\n",
       " 'collaborate': 175,\n",
       " 'collaborated': 176,\n",
       " 'collaboration': 177,\n",
       " 'collaborations': 178,\n",
       " 'collected': 179,\n",
       " 'colorado': 180,\n",
       " 'combination': 181,\n",
       " 'coming': 182,\n",
       " 'commercial': 183,\n",
       " 'communicate': 184,\n",
       " 'communicating': 185,\n",
       " 'communication': 186,\n",
       " 'companies': 187,\n",
       " 'company': 188,\n",
       " 'competitiveness': 189,\n",
       " 'compiled': 190,\n",
       " 'complete': 191,\n",
       " 'completed': 192,\n",
       " 'complex': 193,\n",
       " 'compliance': 194,\n",
       " 'concept': 195,\n",
       " 'concepts': 196,\n",
       " 'concerning': 197,\n",
       " 'concurrent': 198,\n",
       " 'conduct': 199,\n",
       " 'conducted': 200,\n",
       " 'confidential': 201,\n",
       " 'connect': 202,\n",
       " 'consented': 203,\n",
       " 'consultant': 204,\n",
       " 'consulting': 205,\n",
       " 'context': 206,\n",
       " 'continuously': 207,\n",
       " 'contributing': 208,\n",
       " 'conversion': 209,\n",
       " 'conveys': 210,\n",
       " 'coordinated': 211,\n",
       " 'corporation': 212,\n",
       " 'costing': 213,\n",
       " 'council': 214,\n",
       " 'course': 215,\n",
       " 'coursera': 216,\n",
       " 'courses': 217,\n",
       " 'create': 218,\n",
       " 'created': 219,\n",
       " 'createelement': 220,\n",
       " 'creation': 221,\n",
       " 'creativity': 222,\n",
       " 'creator': 223,\n",
       " 'credentials': 224,\n",
       " 'critical': 225,\n",
       " 'cross': 226,\n",
       " 'crrt': 227,\n",
       " 'cruz': 228,\n",
       " 'ct': 229,\n",
       " 'culture': 230,\n",
       " 'cum': 231,\n",
       " 'customer': 232,\n",
       " 'dashboard': 233,\n",
       " 'dashboards': 234,\n",
       " 'datasets': 235,\n",
       " 'date': 236,\n",
       " 'dateposted': 237,\n",
       " 'day': 238,\n",
       " 'dean': 239,\n",
       " 'dec': 240,\n",
       " 'decisions': 241,\n",
       " 'define': 242,\n",
       " 'defined': 243,\n",
       " 'denver': 244,\n",
       " 'depth': 245,\n",
       " 'description': 246,\n",
       " 'descriptions': 247,\n",
       " 'design': 248,\n",
       " 'designated': 249,\n",
       " 'development': 250,\n",
       " 'devices': 251,\n",
       " 'diego': 252,\n",
       " 'direct': 253,\n",
       " 'direction': 254,\n",
       " 'discovered': 255,\n",
       " 'disease': 256,\n",
       " 'dissertation': 257,\n",
       " 'distinguished': 258,\n",
       " 'distractions': 259,\n",
       " 'diverse': 260,\n",
       " 'dna': 261,\n",
       " 'doctoral': 262,\n",
       " 'document': 263,\n",
       " 'documents': 264,\n",
       " 'doing': 265,\n",
       " 'domain': 266,\n",
       " 'draw': 267,\n",
       " 'drive': 268,\n",
       " 'driven': 269,\n",
       " 'dublin': 270,\n",
       " 'dynamics': 271,\n",
       " 'ecls': 272,\n",
       " 'ecmo': 273,\n",
       " 'edinburgh': 274,\n",
       " 'education': 275,\n",
       " 'educational': 276,\n",
       " 'educator': 277,\n",
       " 'educators': 278,\n",
       " 'effectively': 279,\n",
       " 'efforts': 280,\n",
       " 'embedded': 281,\n",
       " 'employmenttype': 282,\n",
       " 'encourages': 283,\n",
       " 'engineering': 284,\n",
       " 'enjoys': 285,\n",
       " 'ensure': 286,\n",
       " 'entered': 287,\n",
       " 'equipment': 288,\n",
       " 'evangelist': 289,\n",
       " 'excited': 290,\n",
       " 'execute': 291,\n",
       " 'executes': 292,\n",
       " 'executive': 293,\n",
       " 'experimental': 294,\n",
       " 'expert': 295,\n",
       " 'experts': 296,\n",
       " 'exploring': 297,\n",
       " 'exposure': 298,\n",
       " 'expression': 299,\n",
       " 'extension': 300,\n",
       " 'extensive': 301,\n",
       " 'external': 302,\n",
       " 'face': 303,\n",
       " 'facilitate': 304,\n",
       " 'facing': 305,\n",
       " 'false': 306,\n",
       " 'family': 307,\n",
       " 'fellow': 308,\n",
       " 'fellowship': 309,\n",
       " 'fellowships': 310,\n",
       " 'field': 311,\n",
       " 'fl': 312,\n",
       " 'fl2002': 313,\n",
       " 'fl2011': 314,\n",
       " 'floating': 315,\n",
       " 'florida': 316,\n",
       " 'flow': 317,\n",
       " 'fluidigm': 318,\n",
       " 'focused': 319,\n",
       " 'follow': 320,\n",
       " 'foundation': 321,\n",
       " 'founded': 322,\n",
       " 'function': 323,\n",
       " 'functional': 324,\n",
       " 'functions': 325,\n",
       " 'funds': 326,\n",
       " 'funnels': 327,\n",
       " 'future': 328,\n",
       " 'ga': 329,\n",
       " 'gaallowlinker': 330,\n",
       " 'gacode': 331,\n",
       " 'gene': 332,\n",
       " 'genetics': 333,\n",
       " 'genomics': 334,\n",
       " 'getelementsbytagname': 335,\n",
       " 'github': 336,\n",
       " 'given': 337,\n",
       " 'global': 338,\n",
       " 'gmail': 339,\n",
       " 'goaling': 340,\n",
       " 'good': 341,\n",
       " 'google': 342,\n",
       " 'googleanalyticsobject': 343,\n",
       " 'governments': 344,\n",
       " 'gpa': 345,\n",
       " 'graduate': 346,\n",
       " 'grant': 347,\n",
       " 'guest': 348,\n",
       " 'guide': 349,\n",
       " 'guidebook': 350,\n",
       " 'haven': 351,\n",
       " 'hay': 352,\n",
       " 'headquartered': 353,\n",
       " 'health': 354,\n",
       " 'healthcare': 355,\n",
       " 'heart': 356,\n",
       " 'help': 357,\n",
       " 'high': 358,\n",
       " 'highlights': 359,\n",
       " 'highly': 360,\n",
       " 'hill': 361,\n",
       " 'hippocampus': 362,\n",
       " 'hiringorganization': 363,\n",
       " 'history': 364,\n",
       " 'hoc': 365,\n",
       " 'home': 366,\n",
       " 'honor': 367,\n",
       " 'honors': 368,\n",
       " 'hospital': 369,\n",
       " 'howard': 370,\n",
       " 'http': 371,\n",
       " 'https': 372,\n",
       " 'human': 373,\n",
       " 'iabp': 374,\n",
       " 'icu': 375,\n",
       " 'identified': 376,\n",
       " 'identifies': 377,\n",
       " 'ii': 378,\n",
       " 'image': 379,\n",
       " 'imagine': 380,\n",
       " 'implements': 381,\n",
       " 'important': 382,\n",
       " 'improve': 383,\n",
       " 'include': 384,\n",
       " 'including': 385,\n",
       " 'individual': 386,\n",
       " 'inference': 387,\n",
       " 'influence': 388,\n",
       " 'influenced': 389,\n",
       " 'influences': 390,\n",
       " 'inform': 391,\n",
       " 'information': 392,\n",
       " 'inherited': 393,\n",
       " 'innovation': 394,\n",
       " 'input': 395,\n",
       " 'insertbefore': 396,\n",
       " 'insight': 397,\n",
       " 'insights': 398,\n",
       " 'institute': 399,\n",
       " 'instructors': 400,\n",
       " 'intensive': 401,\n",
       " 'interacting': 402,\n",
       " 'interpersonal': 403,\n",
       " 'interpretation': 404,\n",
       " 'introduction': 405,\n",
       " 'intuition': 406,\n",
       " 'investigator': 407,\n",
       " 'ireland': 408,\n",
       " 'issues': 409,\n",
       " 'james': 410,\n",
       " 'jan': 411,\n",
       " 'jewell': 412,\n",
       " 'job': 413,\n",
       " 'joblocation': 414,\n",
       " 'jobposting': 415,\n",
       " 'jobs': 416,\n",
       " 'jolla': 417,\n",
       " 'js': 418,\n",
       " 'judgement': 419,\n",
       " 'kappa': 420,\n",
       " 'kathleen': 421,\n",
       " 'key': 422,\n",
       " 'kmuller': 423,\n",
       " 'knit': 424,\n",
       " 'knowledge': 425,\n",
       " 'la': 426,\n",
       " 'lab': 427,\n",
       " 'laboratory': 428,\n",
       " 'lacar': 429,\n",
       " 'lane': 430,\n",
       " 'languages': 431,\n",
       " 'large': 432,\n",
       " 'laude': 433,\n",
       " 'launches': 434,\n",
       " 'leadership': 435,\n",
       " 'learning': 436,\n",
       " 'lectures': 437,\n",
       " 'legacy': 438,\n",
       " 'lever': 439,\n",
       " 'leveraged': 440,\n",
       " 'liaison': 441,\n",
       " 'library': 442,\n",
       " 'life': 443,\n",
       " 'linkedin': 444,\n",
       " 'linkid': 445,\n",
       " 'list': 446,\n",
       " 'lives': 447,\n",
       " 'll': 448,\n",
       " 'local': 449,\n",
       " 'logo': 450,\n",
       " 'logos': 451,\n",
       " 'looker': 452,\n",
       " 'los': 453,\n",
       " 'lung': 454,\n",
       " 'machine': 455,\n",
       " 'maglab': 456,\n",
       " 'magnet': 457,\n",
       " 'magnetic': 458,\n",
       " 'managed': 459,\n",
       " 'management': 460,\n",
       " 'manager': 461,\n",
       " 'managers': 462,\n",
       " 'manuscripts': 463,\n",
       " 'marion': 464,\n",
       " 'market': 465,\n",
       " 'marketing': 466,\n",
       " 'marketplace': 467,\n",
       " 'master': 468,\n",
       " 'mastering': 469,\n",
       " 'matlab': 470,\n",
       " 'matter': 471,\n",
       " 'mcdonnell': 472,\n",
       " 'mechanical': 473,\n",
       " 'medical': 474,\n",
       " 'medicine': 475,\n",
       " 'meetings': 476,\n",
       " 'member': 477,\n",
       " 'memorial': 478,\n",
       " 'memory': 479,\n",
       " 'mentoring': 480,\n",
       " 'mentorship': 481,\n",
       " 'messaging': 482,\n",
       " 'method': 483,\n",
       " 'methods': 484,\n",
       " 'metrics': 485,\n",
       " 'michigan': 486,\n",
       " 'milestones': 487,\n",
       " 'million': 488,\n",
       " 'millions': 489,\n",
       " 'model': 490,\n",
       " 'monitor': 491,\n",
       " 'monitoring': 492,\n",
       " 'multiple': 493,\n",
       " 'national': 494,\n",
       " 'need': 495,\n",
       " 'needs': 496,\n",
       " 'neighborhood': 497,\n",
       " 'network': 498,\n",
       " 'neural': 499,\n",
       " 'neurological': 500,\n",
       " 'neurons': 501,\n",
       " 'neuroplasticity': 502,\n",
       " 'neuroscience': 503,\n",
       " 'news': 504,\n",
       " 'nih': 505,\n",
       " 'non': 506,\n",
       " 'note': 507,\n",
       " 'nsf': 508,\n",
       " 'null': 509,\n",
       " 'nurse': 510,\n",
       " 'nurses': 511,\n",
       " 'nursing': 512,\n",
       " 'offices': 513,\n",
       " 'ongoing': 514,\n",
       " 'online': 515,\n",
       " 'operations': 516,\n",
       " 'org': 517,\n",
       " 'organ': 518,\n",
       " 'organization': 519,\n",
       " 'organizations': 520,\n",
       " 'organize': 521,\n",
       " 'organized': 522,\n",
       " 'orthopedic': 523,\n",
       " 'oversaw': 524,\n",
       " 'owned': 525,\n",
       " 'page': 526,\n",
       " 'pageview': 527,\n",
       " 'pals': 528,\n",
       " 'panel': 529,\n",
       " 'papers': 530,\n",
       " 'parentnode': 531,\n",
       " 'participants': 532,\n",
       " 'passion': 533,\n",
       " 'passions': 534,\n",
       " 'patient': 535,\n",
       " 'patients': 536,\n",
       " 'paulo': 537,\n",
       " 'people': 538,\n",
       " 'percent': 539,\n",
       " 'perform': 540,\n",
       " 'ph': 541,\n",
       " 'phi': 542,\n",
       " 'phil': 543,\n",
       " 'pipeline': 544,\n",
       " 'pis': 545,\n",
       " 'place': 546,\n",
       " 'planetree': 547,\n",
       " 'plotting': 548,\n",
       " 'png': 549,\n",
       " 'populations': 550,\n",
       " 'postaladdress': 551,\n",
       " 'postalcode': 552,\n",
       " 'postdoctoral': 553,\n",
       " 'potential': 554,\n",
       " 'power': 555,\n",
       " 'powered': 556,\n",
       " 'practical': 557,\n",
       " 'practices': 558,\n",
       " 'preceptor': 559,\n",
       " 'precisely': 560,\n",
       " 'precision': 561,\n",
       " 'preparation': 562,\n",
       " 'prepared': 563,\n",
       " 'present': 564,\n",
       " 'presentation': 565,\n",
       " 'presentations': 566,\n",
       " 'presented': 567,\n",
       " 'president': 568,\n",
       " 'primary': 569,\n",
       " 'principle': 570,\n",
       " 'prioritization': 571,\n",
       " 'privately': 572,\n",
       " 'proactively': 573,\n",
       " 'problem': 574,\n",
       " 'procedures': 575,\n",
       " 'process': 576,\n",
       " 'produce': 577,\n",
       " 'product': 578,\n",
       " 'professional': 579,\n",
       " 'proficiency': 580,\n",
       " 'progenitor': 581,\n",
       " 'program': 582,\n",
       " 'programming': 583,\n",
       " 'project': 584,\n",
       " 'projects': 585,\n",
       " 'proofed': 586,\n",
       " 'protected': 587,\n",
       " 'protocols': 588,\n",
       " 'provide': 589,\n",
       " 'provided': 590,\n",
       " 'provides': 591,\n",
       " 'provost': 592,\n",
       " 'psi': 593,\n",
       " 'psychology': 594,\n",
       " 'publications': 595,\n",
       " 'push': 596,\n",
       " 'python': 597,\n",
       " 'quality': 598,\n",
       " 'quantitative': 599,\n",
       " 'questions': 600,\n",
       " 'range': 601,\n",
       " 'rate': 602,\n",
       " 'reactions': 603,\n",
       " 'refer': 604,\n",
       " 'refines': 605,\n",
       " 'regeneration': 606,\n",
       " 'region': 607,\n",
       " 'registered': 608,\n",
       " 'regulating': 609,\n",
       " 'regulatory': 610,\n",
       " 'relevant': 611,\n",
       " 'repair': 612,\n",
       " 'repairs': 613,\n",
       " 'replacement': 614,\n",
       " 'reported': 615,\n",
       " 'representation': 616,\n",
       " 'require': 617,\n",
       " 'requirements': 618,\n",
       " 'requires': 619,\n",
       " 'research': 620,\n",
       " 'researcher': 621,\n",
       " 'resource': 622,\n",
       " 'right': 623,\n",
       " 'rn': 624,\n",
       " 'rna': 625,\n",
       " 'role': 626,\n",
       " 'roles': 627,\n",
       " 'runner': 628,\n",
       " 's3': 629,\n",
       " 'safety': 630,\n",
       " 'salk': 631,\n",
       " 'santa': 632,\n",
       " 'scale': 633,\n",
       " 'scan': 634,\n",
       " 'schedule': 635,\n",
       " 'schema': 636,\n",
       " 'scholarship': 637,\n",
       " 'scicd': 638,\n",
       " 'scientific': 639,\n",
       " 'scientist': 640,\n",
       " 'scientists': 641,\n",
       " 'scope': 642,\n",
       " 'screened': 643,\n",
       " 'script': 644,\n",
       " 'scripted': 645,\n",
       " 'segmenting': 646,\n",
       " 'selected': 647,\n",
       " 'send': 648,\n",
       " 'sept': 649,\n",
       " 'seq': 650,\n",
       " 'sequencing': 651,\n",
       " 'series': 652,\n",
       " 'serve': 653,\n",
       " 'served': 654,\n",
       " 'services': 655,\n",
       " 'set': 656,\n",
       " 'sets': 657,\n",
       " 'setting': 658,\n",
       " 'settings': 659,\n",
       " 'share': 660,\n",
       " 'shared': 661,\n",
       " 'sharp': 662,\n",
       " 'sigma': 663,\n",
       " 'signatures': 664,\n",
       " 'single': 665,\n",
       " 'skilled': 666,\n",
       " 'smith': 667,\n",
       " 'social': 668,\n",
       " 'society': 669,\n",
       " 'soft': 670,\n",
       " 'solutions': 671,\n",
       " 'solving': 672,\n",
       " 'soma': 673,\n",
       " 'south': 674,\n",
       " 'specializing': 675,\n",
       " 'sql': 676,\n",
       " 'src': 677,\n",
       " 'stacks': 678,\n",
       " 'staff': 679,\n",
       " 'staffing': 680,\n",
       " 'stakeholders': 681,\n",
       " 'standard': 682,\n",
       " 'stanford': 683,\n",
       " 'state': 684,\n",
       " 'statistical': 685,\n",
       " 'statistics': 686,\n",
       " 'stem': 687,\n",
       " 'storytelling': 688,\n",
       " 'stratified': 689,\n",
       " 'stroke': 690,\n",
       " 'strong': 691,\n",
       " 'strongly': 692,\n",
       " 'student': 693,\n",
       " 'students': 694,\n",
       " 'studies': 695,\n",
       " 'study': 696,\n",
       " 'subject': 697,\n",
       " 'subjects': 698,\n",
       " 'success': 699,\n",
       " 'summa': 700,\n",
       " 'supervised': 701,\n",
       " 'support': 702,\n",
       " 'supports': 703,\n",
       " 'surgery': 704,\n",
       " 'sweden': 705,\n",
       " 'são': 706,\n",
       " 'tableau': 707,\n",
       " 'talk': 708,\n",
       " 'tallahassee': 709,\n",
       " 'tau': 710,\n",
       " 'teaching': 711,\n",
       " 'team': 712,\n",
       " 'teams': 713,\n",
       " 'teamwork': 714,\n",
       " 'tech': 715,\n",
       " 'technical': 716,\n",
       " 'technicians': 717,\n",
       " 'temporal': 718,\n",
       " 'theta': 719,\n",
       " 'think': 720,\n",
       " 'throughput': 721,\n",
       " 'time': 722,\n",
       " 'title': 723,\n",
       " 'toastmasters': 724,\n",
       " 'today': 725,\n",
       " 'tool': 726,\n",
       " 'tools': 727,\n",
       " 'track': 728,\n",
       " 'tracks': 729,\n",
       " 'tracksidentify': 730,\n",
       " 'trained': 731,\n",
       " 'trainee': 732,\n",
       " 'training': 733,\n",
       " 'transplantation': 734,\n",
       " 'transplants': 735,\n",
       " 'trauma': 736,\n",
       " 'treatment': 737,\n",
       " 'treatments': 738,\n",
       " 'true': 739,\n",
       " 'turkey': 740,\n",
       " 'type': 741,\n",
       " 'ua': 742,\n",
       " 'uc': 743,\n",
       " 'ucla': 744,\n",
       " 'ucsd': 745,\n",
       " 'udemy': 746,\n",
       " 'undergoing': 747,\n",
       " 'undergraduate': 748,\n",
       " 'undergraduates': 749,\n",
       " 'unit': 750,\n",
       " 'units': 751,\n",
       " 'university': 752,\n",
       " 'ups': 753,\n",
       " 'usa': 754,\n",
       " 'use': 755,\n",
       " 'user': 756,\n",
       " 'valve': 757,\n",
       " 'var': 758,\n",
       " 'variety': 759,\n",
       " 'vascular': 760,\n",
       " 'visualization': 761,\n",
       " 'visualizations': 762,\n",
       " 'volunteer': 763,\n",
       " 'white': 764,\n",
       " 'willingness': 765,\n",
       " 'window': 766,\n",
       " 'won': 767,\n",
       " 'workers': 768,\n",
       " 'working': 769,\n",
       " 'world': 770,\n",
       " 'www': 771,\n",
       " 'yale': 772,\n",
       " 'years': 773}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.093151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.081655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.031050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.031050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.122482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.031050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.124201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.122482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.124201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.248402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.081655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.155251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.122482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.124201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.031050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.040827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>0.045142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>0.045142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>0.045142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>0.045142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>0.180568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>0.045142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>0.045142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>0.045142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>0.022571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>0.045142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0.045142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>0.045142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>857 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0    0.040827\n",
       "1    0.093151\n",
       "2    0.081655\n",
       "3    0.040827\n",
       "4    0.040827\n",
       "5    0.040827\n",
       "6    0.040827\n",
       "7    0.031050\n",
       "8    0.031050\n",
       "9    0.040827\n",
       "10   0.040827\n",
       "11   0.040827\n",
       "12   0.122482\n",
       "13   0.040827\n",
       "14   0.031050\n",
       "15   0.124201\n",
       "16   0.122482\n",
       "17   0.124201\n",
       "18   0.040827\n",
       "19   0.248402\n",
       "20   0.081655\n",
       "21   0.062100\n",
       "22   0.040827\n",
       "23   0.155251\n",
       "24   0.122482\n",
       "25   0.040827\n",
       "26   0.040827\n",
       "27   0.124201\n",
       "28   0.031050\n",
       "29   0.040827\n",
       "..        ...\n",
       "827  0.045142\n",
       "828  0.022571\n",
       "829  0.045142\n",
       "830  0.022571\n",
       "831  0.045142\n",
       "832  0.022571\n",
       "833  0.022571\n",
       "834  0.022571\n",
       "835  0.022571\n",
       "836  0.022571\n",
       "837  0.022571\n",
       "838  0.022571\n",
       "839  0.022571\n",
       "840  0.022571\n",
       "841  0.022571\n",
       "842  0.022571\n",
       "843  0.022571\n",
       "844  0.022571\n",
       "845  0.022571\n",
       "846  0.045142\n",
       "847  0.180568\n",
       "848  0.022571\n",
       "849  0.045142\n",
       "850  0.045142\n",
       "851  0.045142\n",
       "852  0.022571\n",
       "853  0.022571\n",
       "854  0.045142\n",
       "855  0.045142\n",
       "856  0.045142\n",
       "\n",
       "[857 rows x 1 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.tocoo().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These tools give single words by default. I think knowing phrases/sentiments would be more useful for me.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP with scikit-learn\n",
    "\n",
    "- Kevin's philosophy is to be really good at scikit-learn and do natural language processing there, rather than being OK at sklearn and OK at NLTK and stitching them together (although I may have to do this)\n",
    "- These are simple techniques and he thinks you can go far with it\n",
    "\n",
    "- NLP means building probabilistic models using data about a language. It might be something like identifying the subject, especially if it's a formal pronoun like a named entity (e.g. probability that a capitalized word is a subject).\n",
    "- You need to understand each language you'll work with and how the world uses it (understanding idioms, sarcasm, etc.)\n",
    "\n",
    "**Note: Natural language processing is not equivalent to machine learning with text. Machine learning with text is one tool in the arsenal of NLP.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher level \"task areas\"\n",
    "- Information retrieval (Google search)\n",
    "- Information from extraction (events from email)\n",
    "- Machine translation (Google translate)\n",
    "- Text simplification (simple Wikipedia)\n",
    "- Predictive text (texting)\n",
    "- Sentiment analysis (Hater News)\n",
    "- Automatic summarization (generating an abstract)\n",
    "- Natural language generation (sports summary)\n",
    "- Speech recognition and generation (speech-to-text)\n",
    "- Question answering (supercomputer Watson on Jeopardy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above can be broken down into lower level \"components\"\n",
    "\n",
    "- Tokenization\n",
    "- Stop word removal\n",
    "- TF-IDF (computing word importance)\n",
    "- Stemming and lemmatization (running > run)\n",
    "- Part-of-speech tagging\n",
    "- Named entity recognition\n",
    "- Segmentation\n",
    "- Word sense disambiguation (\"buy a mouse\")\n",
    "- Spelling correction\n",
    "- Language detection\n",
    "- Machine learningm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "following [this](https://www.youtube.com/watch?v=FLZvOKSCkxY)\n",
    "\n",
    "\n",
    "Terms:\n",
    "- corpora - body of text (medical journals, presidential speeches)\n",
    "- lexicon - words and their meanings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T02:57:37.064214Z",
     "start_time": "2020-09-12T02:57:37.016731Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6e230a00a763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note downloaded to /users/lacar/nltk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-12T03:00:08.473686Z",
     "start_time": "2020-09-12T03:00:08.446937Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7356b9c46687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This can save lots of time versus doing it by regular expressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# This can save lots of time versus doing it by regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wanted to see if and would be listed once or it would list each example\n",
    "example_text = 'Hello Dr. Ben, how are you? The weather is windy and Python is great! I like being alive. and and'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Dr. Ben, how are you?',\n",
       " 'The weather is windy and Python is great!',\n",
       " 'I like being alive.',\n",
       " 'and and']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Dr.',\n",
       " 'Ben',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'windy',\n",
       " 'and',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'great',\n",
       " '!',\n",
       " 'I',\n",
       " 'like',\n",
       " 'being',\n",
       " 'alive',\n",
       " '.',\n",
       " 'and',\n",
       " 'and']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using resume as example for tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BENJAMIN LACAR, Ph.D. 619.419.6227  ben.lacar@gmail.com  www.linkedin.com/in/lacar/ github.com/benslack19   Professional highlights  4+ years as an applications and bioinformatics scientist that influences product development and business messaging through data analysis and visualizations.',\n",
       " 'Skilled in applying Python and R for analysis of high-throughput DNA sequencing data.',\n",
       " 'Adept at communicating scientific and statistical concepts in professional and educational settings with storytelling.',\n",
       " 'Extensive academic background (14 publications) in neuroscience.',\n",
       " 'Skills  R Python MATLAB Statistics Machine Learning Bioinformatics Data Visualization SQL Online Teaching   Professional Experience  Fluidigm Corporation | South San Francisco, CA Bioinformatics Scientist | Genomics R&D                                                                      Jan 2019 - present  Built pipeline for RNA-seq library preparation method, reported metrics, and created ad hoc data visualizations for product development experiments.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(resume_text_frdocx)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not super helpful - just a way to import stop words and then you can filter them out using a list comprehension\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure why it has to be a set\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note this is much more than video example, because it's been updated\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others:\n",
    "https://www.youtube.com/watch?v=w36-U-ccajM&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL&index=2\n",
    "    \n",
    "Chunking, chinking, named entity recognition, lemmatizing, NLTK Corpora, wordnet, text classification, words as features for learning, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "apple + purple = plum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations to fill skills gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_subject = 'SQL'\n",
    "url = 'https://www.coursera.org/search?query=' + query_subject\n",
    "# socket = urllib.request.urlopen(url)\n",
    "document = urllib.request.urlopen(url).read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
